#%% 
import tensorflow as tf

#---------------------------- train data -------------------------------
#%% read data
with open('E:\datasets\qi18naacl-dataset\datasets\pt_to_en\en.train', 'r', encoding='utf8') as f:
    en = f.readlines()
    
with open('E:\datasets\qi18naacl-dataset\datasets\pt_to_en\pt.train', 'r', encoding='utf8') as f:
    pt = f.readlines()
    
print('English version has {} sentences'.format(len(en)))
print('Portuguese version has {} sentences'.format(len(pt)))
print('\n')
print('The first sentence of the English version is:\n{}'.format(en[0]))
print('The first sentence of the Portuguese version is:\n{}'.format(pt[0]))

#%% add start and end token
def add_start_end(x):
    '''
    INPUT
        x is a string
    '''
    return '<START> ' + x + ' <END>'

en = [add_start_end(x) for x in en]
pt = [add_start_end(x) for x in pt]

#%% tokenize text
tokenizer_en = tf.keras.preprocessing.text.Tokenizer()
tokenizer_en.fit_on_texts(en)

tokenizer_pt = tf.keras.preprocessing.text.Tokenizer()
tokenizer_pt.fit_on_texts(pt)

# word token to integers
en_seq = tokenizer_en.texts_to_sequences(en)
pt_seq = tokenizer_pt.texts_to_sequences(pt)

# pad sequences to have the same length
en_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(en_seq)
pt_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(pt_seq)

#%% build dataset
batch_size = 32
train_dataset = tf.data.Dataset.from_tensor_slices((en_seq_padded, pt_seq_padded)).shuffle(51785).batch(batch_size)

#------------------------------ dev data ---------------------------------
#%% read dev data
with open('E:\datasets\qi18naacl-dataset\datasets\pt_to_en\en.dev', 'r', encoding='utf8') as f:
    en_dev = f.readlines()
    
with open('E:\datasets\qi18naacl-dataset\datasets\pt_to_en\pt.dev', 'r', encoding='utf8') as f:
    pt_dev = f.readlines()
    
print('English version has {} sentences'.format(len(en_dev)))
print('Portuguese version has {} sentences'.format(len(pt_dev)))
print('\n')
print('The first sentence of the English version is:\n{}'.format(en_dev[0]))
print('The first sentence of the Portuguese version is:\n{}'.format(pt_dev[0]))

#%% add start and end token
en_dev = [add_start_end(x) for x in en_dev]
pt_dev = [add_start_end(x) for x in pt_dev]

#%% tokenize
en_dev_seq = tokenizer_en.texts_to_sequences(en_dev)
pt_dev_seq = tokenizer_pt.texts_to_sequences(pt_dev)

#%% padding
en_dev_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(en_dev_seq)
pt_dev_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(pt_dev_seq)

#-------------------------- modelling ------------------------------
def scaled_dotted_product_attention(Q, K, V, mask=None):
    '''
    INPUT
        Q      [batch_size, seq_length, depth]
        K      [batch_size, seq_length, depth]
        V      [batch_size, seq_length, depth]
        mask   [batch_size, seq_length, seq_length]
        depth  the dimension of each head, should be d_model / head_num 
    OUTPUT
        context    [batch_size, seq_length, depth]
    '''
    QK = tf.matmul(Q, K, transpose_b=True) # [batch_size, seq_length, seq_length]
    QK = QK / tf.math.sqrt(tf.cast(Q.shape[-1], tf.float32)) # [batch_size, seq_length, seq_length]
    
    # mask out future information
    if mask is not None:
        QK += (mask * 1e-9)
        
    attention_weights = tf.math.softmax(QK, axis=-1) # [batch_size, seq_length, seq_length]
    
    outputs = tf.matmul(attention_weights, V)
    return outputs
    
class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        '''
        INPUT
            d_model      dimension of the encoded representations, not the embedding dim  
            num_heads    how many different spaces for self attention
        '''
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        
        assert d_model % num_heads == 0
        
        self.depth = d_model // num_heads
        
        self.query_dense = tf.keras.layers.Dense(self.d_model)
        self.key_dense = tf.keras.layers.Dense(self.d_model)
        self.value_dense = tf.keras.layers.Dense(self.d_model)
        
        self.fc = tf.keras.layers.Dense(self.d_model)
        
    def split_heads(self, X):
        '''
        INPUT
            X     a tensor, [batch_size, seq_length, d_model]
        OUTPUT
            X     [batch_size, num_heads, seq_length, depth]
                  d_model = num_heads * depth
        '''
        batch_size, seq_length = X.shape[0], X.shape[1]
        X = tf.reshape(X, [batch_size, seq_length, self.num_heads, self.d_model//self.num_heads])
        return tf.transpose(X, [0,2,1,3])
        
    def call(self, inputs):
        '''
        INPUT
            inputs is a dictionary so we can use funcitonal api
        OUTPUT
            context    [batch_size, seq_len, d_model]
        '''
        Q, K, V, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']
        batch_size, seq_len = Q.shape[0], Q.shape[1]
        
        Q = self.query_dense(Q) # [batch_size, seq_length, d_model]
        K = self.key_dense(K)
        V = self.value_dense(V)
        
        Q = self.split_heads(Q) # [batch_size, num_heads, seq_length, depth]
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        context = scaled_dotted_product_attention(Q, K, V, mask) # [batch_size, num_heads, seq_length, depth]
        
        # concat all heads together
        context = tf.transpose(context, [0,2,1,3]) # [batch_size, seq_length, num_heads, depth]
        context = tf.reshape(context, shape = [batch_size, seq_len, self.d_model]) # [batch_size, seq_length, d_model]
        
        context = self.fc(context)
        
        return context 
    
# use tensorflow funtional api to have compact code
def encoder_sublayer(d_model, num_heads, dropout_rate):
    inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
    mask = tf.keras.Input(shape=(1,1,seq_len), name='mask')
    
    context = MultiHeadAttention(d_model, num_heads)(
        {'key':inputs,
         'query':inputs,
         'value':inputs,
         'mask':mask
        }
    )
    
    context = tf.keras.layers.Dropout(dropout_rate)(context)
    context = tf.keras.layers.LayerNormalization(epsilon=1e-6)(context)
    
    outputs = tf.keras.layers.Dense(d_model)(context)
    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)
    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + context)
    
    return tf.keras.Model(inputs = [inputs, mask], outputs = outputs, name = 'encoder_sublayer')

        
def positional_encoding(pos, d_embed):
    '''
    INPUT
        pos         should be seq_len
        d_embed     integer, embedding dimension
    '''
    assert d_embed % 2 == 0
    angle = tf.range(d_embed // 2, dtype=tf.float32)
    angle = tf.range(pos, dtype = tf.float32)[:,tf.newaxis] / tf.pow(1e4, 2 * angle / d_embed)[tf.newaxis,:]
    encoded_0 = tf.math.sin(angle)[:,:,tf.newaxis] #[seq_len, d_embed // 2, 1]
    encoded_1 = tf.math.cos(angle)[:,:,tf.newaxis] 

    encoded = tf.concat([encoded_0, encoded_1], axis=-1) #[seq_len, d_embed // 2, 2]
    encoded = tf.reshape(encoded, shape = [-1,d_embed]) 
    return encoded


def encoder_model(vocab_size, seq_length, num_layers, d_model, num_heads, dropout_rate):
    inputs = tf.keras.Input(shape=(None, seq_length), name='inputs')
    mask = tf.keras.Input(shape=(1,1,None), name='mask')
    
    x = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
    
    x = positional_encoding(seq_length, d_model) + x
    x = tf.keras.layers.Dropout(dropout_rate)(x)
    
    for _ in range(num_layers):
        x = encoder_sublayer(d_model, num_heads, dropout_rate)([x, mask])
    
    return tf.keras.Model(inputs = [inputs, mask], outputs = x, name='transformer_encoders')


len(tokenizer_en.word_counts)
